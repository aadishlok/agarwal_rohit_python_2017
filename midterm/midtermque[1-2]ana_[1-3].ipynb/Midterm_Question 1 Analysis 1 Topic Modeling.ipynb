{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "\n",
    "- Topic Modelling on All Employees Sent Emails\n",
    "- What is Topic Modelling?\n",
    "  - Topic can be defined as “a repeating pattern of co-occurring terms in a corpus”.\n",
    "- What is Ernon Scandal?\n",
    "   - Enron Corporation was an American energy, commodities, and services company based in Houston, Texas.\n",
    "     Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and wasone of the world's \n",
    "     major electricity, natural gas, communications, and pulp and paper companies, with claimed revenues of nearly 111 billion \n",
    "     during 2000. At the end of 2001, it was revealed that its reported financial condition was sustained substantially \n",
    "     by an institutionalized, systematic, and creatively planned accounting fraud, known since as the Enron scandal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import email\n",
    "from email.parser import Parser\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 \n",
    "Data Collection & Pre Processing\n",
    "\n",
    "- Initially the data was in MIME type format.\n",
    "- Lopped through each email and sent to Email Parser (get_payload()) in order to get the \"Text Body\".\n",
    "- Stored text body of all sent emails as txt file in seperate directory. [midterm/data/Topic Modelling/Sent Emails/*.txt].\n",
    "- Read all sent emails txt files and stored in one LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relativePath = os.getcwd()\n",
    "path = relativePath+\"/\"+'midterm/data/ernon/maildir/'\n",
    "i = 1;\n",
    "\n",
    "if os.path.isdir(relativePath+\"/\"+'midterm/data/') == 1:\n",
    "    if os.path.isdir(relativePath+\"/\"+'midterm/data/Topic Modeling/Sent Emails') == 0:\n",
    "        os.makedirs(relativePath+\"/\"+'midterm/data/Topic Modeling/Sent Emails')\n",
    "\n",
    "# Using Email Parder to read MIME type emails.\n",
    "        \n",
    "def emailParser(inputFile, i):\n",
    "    with open(inputFile, \"r\") as f:\n",
    "        data = f.read()\n",
    "    email = Parser().parsestr(data)\n",
    "    with open(relativePath+\"/\"+'midterm/data/Topic Modeling/Sent Emails/'+str(i)+'.txt', 'w', encoding='utf-8') as txtFile:\n",
    "        txtFile.write(email.get_payload())\n",
    "\n",
    "\n",
    "for directory, subDirectory, fileNames in os.walk(path):\n",
    "    if 'sent_items' in directory:\n",
    "        for filename in fileNames:\n",
    "            emailParser(os.path.join(directory, filename), i)\n",
    "            i=i+1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_com = []\n",
    "\n",
    "def readEmail(inputFile):\n",
    "    with open(inputFile, \"r\") as f:\n",
    "        data = f.read()\n",
    "        doc_com.append(data)\n",
    "\n",
    "\n",
    "for directory, subDirectory, fileNames in os.walk(relativePath+\"/\"+'midterm/data/Topic Modeling/Sent Emails/'):\n",
    "    for files in fileNames:\n",
    "        readEmail(os.path.join(directory, files))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "- Cleaning\n",
    "- Looped through the List to remove all stopword using (nltk stopwords('english))\n",
    "- Removed all Punctuation from the List.\n",
    "- Normalized the Data List using (NLTK wordnet lemma.lemmatize(word))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_com]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "- Making Document Term Matrix\n",
    "\n",
    "- All the text documents combined is known as the corpus. To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix.\n",
    "\n",
    "- Creating the term dictionary of data, where every unique term is assigned an index. \n",
    "- Converting list of documents (corpus) into Document Term Matrix using dictionary prepared.\n",
    "- Created an object for LDA model and train it on Document-Term matrix using gensim library(Lda = gensim.models.ldamodel.LdaModel).\n",
    "- Running and Trainign LDA model on the document term matrix using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=4, id2word = dictionary, passes=50)\n",
    "\n",
    "tList = ldamodel.print_topics(num_topics=4, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "- Result of Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic ['would', 'enron', 'power', 'company', 'market', 'energy', 'price', 'gas', 'contract']\n",
      "Topic ['intended', 'email', 'recipient', 'omnicalendarentry', 'or', 'corp', 'use', 'affiliate', 'enron', 'omniexcludefromviewdomniexcludefromview']\n",
      "Topic ['message', 'subject', 'to', 'from', 'original', 'sent', 'please', 'pm', 'cc']\n",
      "Topic ['message', 'to', 'sent', 'subject', 'from', 'original', 'pm', 're', 'know']\n"
     ]
    }
   ],
   "source": [
    "topicList = []\n",
    "\n",
    "for x in tList:\n",
    "    y = re.sub('[^A-Za-z]+', ' ', x[1])\n",
    "    topicList.append(y)\n",
    "    \n",
    "\n",
    "for z in topicList:\n",
    "    seperate = z.split()\n",
    "    print('Topic', seperate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is a topic with individual topic terms. \n",
    "- Topic 1 - It Can be termed as Business.\n",
    "- Topic 2 - It Can be termed as Legalities.\n",
    "- Topic 3 - It Can be termed as Meeting.\n",
    "- Topic 4 - It Can be termed as Meeting in casual tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Topic 1 contains words that are directly related to the core business of Enron like \"gas\", \"power\" etc.\n",
    "- Topic 2 while related to business seems to be more about the process rather than the content of the core business. It has a lot of terms relevant to business legalities.\n",
    "- Topic 3 contains a lot of meeting related words, perhaps they are from emails that were sent as meeting notices.\n",
    "- Topic 4 also seems to be meeting-related but in a more casual tone and setting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
